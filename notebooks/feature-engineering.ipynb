{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c957d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bbfb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING FOR FRAUD DETECTION\n",
      "================================================================================\n",
      "\n",
      "✓ Fraud data loaded: (151112, 14)\n",
      "✓ Credit card data loaded: (284807, 32)\n",
      "✓ IP data loaded: (138846, 3)\n"
     ]
    }
   ],
   "source": [
    "# ==================== LOAD DATA ====================\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING FOR FRAUD DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load explored data\n",
    "\n",
    "fraud_data = pd.read_csv('../data/processed/fraud_data_explored.csv')\n",
    "creditcard = pd.read_csv('../data/processed/creditcard_explored.csv')\n",
    "ip_data = pd.read_csv('../data/raw/ipAddress_to_Country.csv')\n",
    "\n",
    "print(f\"\\n✓ Fraud data loaded: {fraud_data.shape}\")\n",
    "print(f\"✓ Credit card data loaded: {creditcard.shape}\")\n",
    "print(f\"✓ IP data loaded: {ip_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed46d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PART 1: E-COMMERCE FRAUD DATA - FEATURE ENGINEERING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==================== FRAUD DATA PREPROCESSING ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1: E-COMMERCE FRAUD DATA - FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert datetime\n",
    "fraud_data['signup_time'] = pd.to_datetime(fraud_data['signup_time'])\n",
    "fraud_data['purchase_time'] = pd.to_datetime(fraud_data['purchase_time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9a454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. GEOLOCATION INTEGRATION\n",
      "--------------------------------------------------------------------------------\n",
      "Converting IP addresses to integers...\n",
      "✓ Converted 0 IP addresses\n",
      "  Data type: float64\n",
      "\n",
      "Preparing IP lookup table...\n",
      "  Found columns: lower_bound_ip_address, upper_bound_ip_address\n",
      "  Using existing numeric values...\n",
      "✓ IP lookup table prepared\n",
      "  lower_bound dtype: float64\n",
      "  upper_bound dtype: int64\n",
      "  Removed 151112 invalid IPs from fraud data\n",
      "✓ Valid IP integers: 0\n",
      "\n",
      "Verifying data types for merge:\n",
      "  Left key (ip_integer): float64\n",
      "  Right key (lower_bound): float64\n",
      "\n",
      "Sorting datasets for merge...\n",
      "\n",
      "Performing range-based merge (this may take a moment)...\n",
      "✓ Merge successful!\n",
      "\n",
      "Validating merge results...\n",
      "  Rows before validation: 0\n",
      "  Rows after validation: 0\n",
      "  ⚠ Added 151,112 unmatched transactions as 'Unknown'\n",
      "\n",
      "✓ Final statistics:\n",
      "  Total transactions: 151,112\n",
      "  Matched to countries: 0 (0.00%)\n",
      "  Unknown countries: 151,112 (100.00%)\n",
      "\n",
      "Creating country-based features...\n",
      "✓ Country fraud rate feature created\n",
      "✓ Identified 0 high-risk countries\n",
      "  Average fraud rate: 0.0936\n",
      "\n",
      "Top 10 countries by fraud rate:\n"
     ]
    }
   ],
   "source": [
    "# ==================== 1. GEOLOCATION FEATURES ====================\n",
    "print(\"\\n1. GEOLOCATION INTEGRATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def ip_to_integer(ip_str):\n",
    "    \"\"\"Convert IP address to integer\"\"\"\n",
    "    try:\n",
    "        parts = str(ip_str).split('.')\n",
    "        return int(parts[0]) * 256**3 + int(parts[1]) * 256**2 + \\\n",
    "               int(parts[2]) * 256 + int(parts[3])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Convert IP addresses\n",
    "print(\"Converting IP addresses to integers...\")\n",
    "fraud_data['ip_integer'] = fraud_data['ip_address'].apply(ip_to_integer)\n",
    "\n",
    "# CRITICAL FIX: Convert to numeric dtype for merge_asof\n",
    "fraud_data['ip_integer'] = pd.to_numeric(fraud_data['ip_integer'], errors='coerce')\n",
    "\n",
    "print(f\"✓ Converted {fraud_data['ip_integer'].notna().sum():,} IP addresses\")\n",
    "print(f\"  Data type: {fraud_data['ip_integer'].dtype}\")\n",
    "\n",
    "# Prepare IP data and ensure numeric types\n",
    "print(\"\\nPreparing IP lookup table...\")\n",
    "\n",
    "# Handle different possible column names\n",
    "if 'lower_bound_ip_address' in ip_data.columns and 'upper_bound_ip_address' in ip_data.columns:\n",
    "    print(\"  Found columns: lower_bound_ip_address, upper_bound_ip_address\")\n",
    "    \n",
    "    # Check if they're strings (IP format) or already integers\n",
    "    if ip_data['lower_bound_ip_address'].dtype == 'object':\n",
    "        print(\"  Converting IP addresses from string format...\")\n",
    "        ip_data['lower_bound'] = ip_data['lower_bound_ip_address'].apply(ip_to_integer)\n",
    "        ip_data['upper_bound'] = ip_data['upper_bound_ip_address'].apply(ip_to_integer)\n",
    "    else:\n",
    "        print(\"  Using existing numeric values...\")\n",
    "        ip_data['lower_bound'] = ip_data['lower_bound_ip_address']\n",
    "        ip_data['upper_bound'] = ip_data['upper_bound_ip_address']\n",
    "        \n",
    "elif 'lower_bound' in ip_data.columns and 'upper_bound' in ip_data.columns:\n",
    "    print(\"  Found columns: lower_bound, upper_bound\")\n",
    "    # They might already be correct\n",
    "else:\n",
    "    print(\"  ERROR: Could not find IP range columns!\")\n",
    "    print(f\"  Available columns: {ip_data.columns.tolist()}\")\n",
    "    raise ValueError(\"IP range columns not found in ip_data\")\n",
    "\n",
    "# Convert to numeric (handles any remaining string issues)\n",
    "ip_data['lower_bound'] = pd.to_numeric(ip_data['lower_bound'], errors='coerce')\n",
    "ip_data['upper_bound'] = pd.to_numeric(ip_data['upper_bound'], errors='coerce')\n",
    "\n",
    "print(f\"✓ IP lookup table prepared\")\n",
    "print(f\"  lower_bound dtype: {ip_data['lower_bound'].dtype}\")\n",
    "print(f\"  upper_bound dtype: {ip_data['upper_bound'].dtype}\")\n",
    "\n",
    "# Remove invalid entries\n",
    "initial_ip_count = len(ip_data)\n",
    "ip_data = ip_data.dropna(subset=['lower_bound', 'upper_bound'])\n",
    "if initial_ip_count > len(ip_data):\n",
    "    print(f\"  Removed {initial_ip_count - len(ip_data)} invalid IP ranges\")\n",
    "\n",
    "# Remove invalid fraud data IPs\n",
    "initial_fraud_count = len(fraud_data)\n",
    "fraud_data_valid = fraud_data.dropna(subset=['ip_integer']).copy()\n",
    "if initial_fraud_count > len(fraud_data_valid):\n",
    "    print(f\"  Removed {initial_fraud_count - len(fraud_data_valid)} invalid IPs from fraud data\")\n",
    "\n",
    "print(f\"✓ Valid IP integers: {len(fraud_data_valid):,}\")\n",
    "\n",
    "# Verify dtypes before merge\n",
    "print(\"\\nVerifying data types for merge:\")\n",
    "print(f\"  Left key (ip_integer): {fraud_data_valid['ip_integer'].dtype}\")\n",
    "print(f\"  Right key (lower_bound): {ip_data['lower_bound'].dtype}\")\n",
    "\n",
    "# Sort for merge\n",
    "print(\"\\nSorting datasets for merge...\")\n",
    "ip_data_sorted = ip_data.sort_values('lower_bound').copy()\n",
    "fraud_data_sorted = fraud_data_valid.sort_values('ip_integer').copy()\n",
    "\n",
    "# Merge using range lookup\n",
    "print(\"\\nPerforming range-based merge (this may take a moment)...\")\n",
    "try:\n",
    "    merged = pd.merge_asof(\n",
    "        fraud_data_sorted,\n",
    "        ip_data_sorted[['lower_bound', 'upper_bound', 'country']],\n",
    "        left_on='ip_integer',\n",
    "        right_on='lower_bound',\n",
    "        direction='backward'\n",
    "    )\n",
    "    print(\"✓ Merge successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Merge failed with error: {e}\")\n",
    "    print(\"\\nTroubleshooting info:\")\n",
    "    print(f\"  Sample ip_integer values: {fraud_data_sorted['ip_integer'].head(3).tolist()}\")\n",
    "    print(f\"  Sample lower_bound values: {ip_data_sorted['lower_bound'].head(3).tolist()}\")\n",
    "    raise\n",
    "\n",
    "# Validate: keep only rows where IP is within range\n",
    "print(\"\\nValidating merge results...\")\n",
    "print(f\"  Rows before validation: {len(merged):,}\")\n",
    "merged_valid = merged[merged['ip_integer'] <= merged['upper_bound']].copy()\n",
    "print(f\"  Rows after validation: {len(merged_valid):,}\")\n",
    "\n",
    "# Add back rows that couldn't be matched\n",
    "unmatched_indices = fraud_data[~fraud_data.index.isin(merged_valid.index)].index\n",
    "if len(unmatched_indices) > 0:\n",
    "    unmatched = fraud_data.loc[unmatched_indices].copy()\n",
    "    unmatched['country'] = 'Unknown'\n",
    "    unmatched['lower_bound'] = np.nan\n",
    "    unmatched['upper_bound'] = np.nan\n",
    "    \n",
    "    # Ensure all columns exist in unmatched\n",
    "    for col in merged_valid.columns:\n",
    "        if col not in unmatched.columns:\n",
    "            unmatched[col] = np.nan\n",
    "    \n",
    "    # Combine\n",
    "    merged_final = pd.concat([merged_valid, unmatched], ignore_index=True)\n",
    "    print(f\"  ⚠ Added {len(unmatched):,} unmatched transactions as 'Unknown'\")\n",
    "else:\n",
    "    merged_final = merged_valid\n",
    "\n",
    "# Fill any remaining nulls in country\n",
    "merged_final['country'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Statistics\n",
    "matched_count = (merged_final['country'] != 'Unknown').sum()\n",
    "unknown_count = (merged_final['country'] == 'Unknown').sum()\n",
    "\n",
    "print(f\"\\n✓ Final statistics:\")\n",
    "print(f\"  Total transactions: {len(merged_final):,}\")\n",
    "print(f\"  Matched to countries: {matched_count:,} ({matched_count/len(merged_final)*100:.2f}%)\")\n",
    "print(f\"  Unknown countries: {unknown_count:,} ({unknown_count/len(merged_final)*100:.2f}%)\")\n",
    "\n",
    "# Country-based features\n",
    "print(\"\\nCreating country-based features...\")\n",
    "\n",
    "# Country fraud rate\n",
    "country_fraud_rate = merged_final.groupby('country')['class'].mean()\n",
    "merged_final['country_fraud_rate'] = merged_final['country'].map(country_fraud_rate)\n",
    "\n",
    "# Country transaction count\n",
    "country_tx_count = merged_final.groupby('country')['class'].count()\n",
    "merged_final['country_tx_count'] = merged_final['country'].map(country_tx_count)\n",
    "\n",
    "# High-risk countries (fraud rate > average)\n",
    "avg_fraud_rate = merged_final['class'].mean()\n",
    "high_risk_countries = country_fraud_rate[country_fraud_rate > avg_fraud_rate].index\n",
    "merged_final['is_high_risk_country'] = merged_final['country'].isin(high_risk_countries).astype(int)\n",
    "\n",
    "print(f\"✓ Country fraud rate feature created\")\n",
    "print(f\"✓ Identified {len(high_risk_countries)} high-risk countries\")\n",
    "print(f\"  Average fraud rate: {avg_fraud_rate:.4f}\")\n",
    "\n",
    "# Display top risky countries\n",
    "print(\"\\nTop 10 countries by fraud rate:\")\n",
    "top_risky = country_fraud_rate.sort_values(ascending=False).head(10)\n",
    "for country, rate in top_risky.items():\n",
    "    if country != 'Unknown':\n",
    "        count = country_tx_count[country]\n",
    "        print(f\"  {country}: {rate:.4f} ({count:,} transactions)\")\n",
    "\n",
    "fraud_data = merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa1d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. TIME-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: hour_of_day, day_of_week, day_of_month, month\n",
      "✓ Created: time period indicators (night, morning, afternoon, evening, weekend)\n",
      "✓ Created: cyclical time encodings (hour_sin, hour_cos, day_sin, day_cos)\n",
      "✓ Created: time_since_signup (hours and days)\n",
      "✓ Created: account age categories and quick purchase flags\n"
     ]
    }
   ],
   "source": [
    "# ==================== 2. TIME-BASED FEATURES ====================\n",
    "print(\"\\n2. TIME-BASED FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Basic time features\n",
    "fraud_data['hour_of_day'] = fraud_data['purchase_time'].dt.hour\n",
    "fraud_data['day_of_week'] = fraud_data['purchase_time'].dt.dayofweek\n",
    "fraud_data['day_of_month'] = fraud_data['purchase_time'].dt.day\n",
    "fraud_data['month'] = fraud_data['purchase_time'].dt.month\n",
    "fraud_data['year'] = fraud_data['purchase_time'].dt.year\n",
    "\n",
    "print(\"✓ Created: hour_of_day, day_of_week, day_of_month, month\")\n",
    "\n",
    "# Time periods\n",
    "fraud_data['is_night'] = fraud_data['hour_of_day'].between(0, 6).astype(int)\n",
    "fraud_data['is_morning'] = fraud_data['hour_of_day'].between(6, 12).astype(int)\n",
    "fraud_data['is_afternoon'] = fraud_data['hour_of_day'].between(12, 18).astype(int)\n",
    "fraud_data['is_evening'] = fraud_data['hour_of_day'].between(18, 24).astype(int)\n",
    "fraud_data['is_weekend'] = fraud_data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"✓ Created: time period indicators (night, morning, afternoon, evening, weekend)\")\n",
    "\n",
    "# Cyclical encoding for hour and day\n",
    "fraud_data['hour_sin'] = np.sin(2 * np.pi * fraud_data['hour_of_day'] / 24)\n",
    "fraud_data['hour_cos'] = np.cos(2 * np.pi * fraud_data['hour_of_day'] / 24)\n",
    "fraud_data['day_sin'] = np.sin(2 * np.pi * fraud_data['day_of_week'] / 7)\n",
    "fraud_data['day_cos'] = np.cos(2 * np.pi * fraud_data['day_of_week'] / 7)\n",
    "\n",
    "print(\"✓ Created: cyclical time encodings (hour_sin, hour_cos, day_sin, day_cos)\")\n",
    "\n",
    "# Time since signup\n",
    "fraud_data['time_since_signup_hours'] = (fraud_data['purchase_time'] - \n",
    "                                         fraud_data['signup_time']).dt.total_seconds() / 3600\n",
    "fraud_data['time_since_signup_days'] = fraud_data['time_since_signup_hours'] / 24\n",
    "\n",
    "print(\"✓ Created: time_since_signup (hours and days)\")\n",
    "\n",
    "# Account age categories\n",
    "fraud_data['account_age_category'] = pd.cut(\n",
    "    fraud_data['time_since_signup_days'],\n",
    "    bins=[-1, 1, 7, 30, 90, 180, 365, np.inf],\n",
    "    labels=['<1day', '1-7days', '1-4weeks', '1-3months', '3-6months', '6-12months', '>1year']\n",
    ")\n",
    "\n",
    "# Quick purchase flag (purchased within 1 hour of signup)\n",
    "fraud_data['is_quick_purchase'] = (fraud_data['time_since_signup_hours'] < 1).astype(int)\n",
    "fraud_data['is_very_new_account'] = (fraud_data['time_since_signup_days'] < 1).astype(int)\n",
    "\n",
    "print(\"✓ Created: account age categories and quick purchase flags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1bab0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. TRANSACTION FREQUENCY & VELOCITY FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: user_transaction_count\n",
      "✓ Created: time_since_last_tx, is_rapid_tx\n",
      "✓ Created: user purchase statistics and anomaly flags\n"
     ]
    }
   ],
   "source": [
    "# ==================== 3. TRANSACTION FREQUENCY & VELOCITY ====================\n",
    "print(\"\\n3. TRANSACTION FREQUENCY & VELOCITY FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Sort by user and time\n",
    "fraud_data = fraud_data.sort_values(['user_id', 'purchase_time'])\n",
    "\n",
    "# Transaction count per user\n",
    "fraud_data['user_transaction_count'] = fraud_data.groupby('user_id').cumcount() + 1\n",
    "print(\"✓ Created: user_transaction_count\")\n",
    "\n",
    "# Time between transactions\n",
    "fraud_data['time_since_last_tx'] = fraud_data.groupby('user_id')['purchase_time'].diff().dt.total_seconds() / 3600\n",
    "fraud_data['time_since_last_tx'].fillna(999, inplace=True)  # First transaction\n",
    "\n",
    "# Rapid transaction flag\n",
    "fraud_data['is_rapid_tx'] = (fraud_data['time_since_last_tx'] < 1).astype(int)\n",
    "\n",
    "print(\"✓ Created: time_since_last_tx, is_rapid_tx\")\n",
    "\n",
    "# Purchase value patterns per user\n",
    "fraud_data['user_avg_purchase'] = fraud_data.groupby('user_id')['purchase_value'].transform('mean')\n",
    "fraud_data['user_std_purchase'] = fraud_data.groupby('user_id')['purchase_value'].transform('std')\n",
    "fraud_data['user_max_purchase'] = fraud_data.groupby('user_id')['purchase_value'].transform('max')\n",
    "fraud_data['user_min_purchase'] = fraud_data.groupby('user_id')['purchase_value'].transform('min')\n",
    "\n",
    "# Purchase value ratio\n",
    "fraud_data['purchase_value_ratio'] = fraud_data['purchase_value'] / (fraud_data['user_avg_purchase'] + 0.01)\n",
    "fraud_data['is_unusual_amount'] = (abs(fraud_data['purchase_value_ratio']) > 2).astype(int)\n",
    "\n",
    "print(\"✓ Created: user purchase statistics and anomaly flags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3426b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. DEVICE & BROWSER DIVERSITY FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: device/browser diversity features\n",
      "✓ Created: device/browser/source fraud rate features\n",
      "\n",
      "5. USER BEHAVIOR FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: geographic diversity features\n",
      "\n",
      "6. AGE-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: age groups and age-based risk scores\n"
     ]
    }
   ],
   "source": [
    "# ==================== 4. DEVICE & BROWSER FEATURES ====================\n",
    "print(\"\\n4. DEVICE & BROWSER DIVERSITY FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Multiple devices per user\n",
    "fraud_data['user_device_count'] = fraud_data.groupby('user_id')['device_id'].transform('nunique')\n",
    "fraud_data['user_browser_count'] = fraud_data.groupby('user_id')['browser'].transform('nunique')\n",
    "fraud_data['is_multiple_devices'] = (fraud_data['user_device_count'] > 1).astype(int)\n",
    "fraud_data['is_multiple_browsers'] = (fraud_data['user_browser_count'] > 1).astype(int)\n",
    "\n",
    "print(\"✓ Created: device/browser diversity features\")\n",
    "\n",
    "# Device and browser fraud rates\n",
    "device_fraud_rate = fraud_data.groupby('device_id')['class'].mean()\n",
    "browser_fraud_rate = fraud_data.groupby('browser')['class'].mean()\n",
    "source_fraud_rate = fraud_data.groupby('source')['class'].mean()\n",
    "\n",
    "fraud_data['device_fraud_rate'] = fraud_data['device_id'].map(device_fraud_rate)\n",
    "fraud_data['browser_fraud_rate'] = fraud_data['browser'].map(browser_fraud_rate)\n",
    "fraud_data['source_fraud_rate'] = fraud_data['source'].map(source_fraud_rate)\n",
    "\n",
    "print(\"✓ Created: device/browser/source fraud rate features\")\n",
    "\n",
    "# ==================== 5. USER BEHAVIOR FEATURES ====================\n",
    "print(\"\\n5. USER BEHAVIOR FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Multiple countries per user\n",
    "fraud_data['user_country_count'] = fraud_data.groupby('user_id')['country'].transform('nunique')\n",
    "fraud_data['is_multi_country'] = (fraud_data['user_country_count'] > 1).astype(int)\n",
    "\n",
    "# IP changes\n",
    "fraud_data['user_ip_count'] = fraud_data.groupby('user_id')['ip_address'].transform('nunique')\n",
    "fraud_data['is_multi_ip'] = (fraud_data['user_ip_count'] > 1).astype(int)\n",
    "\n",
    "print(\"✓ Created: geographic diversity features\")\n",
    "\n",
    "# ==================== 6. AGE-BASED FEATURES ====================\n",
    "print(\"\\n6. AGE-BASED FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Age groups\n",
    "fraud_data['age_group'] = pd.cut(\n",
    "    fraud_data['age'],\n",
    "    bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "    labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    ")\n",
    "\n",
    "# Age-based risk\n",
    "age_fraud_rate = fraud_data.groupby('age_group')['class'].mean()\n",
    "fraud_data['age_group_fraud_rate'] = fraud_data['age_group'].map(age_fraud_rate)\n",
    "\n",
    "print(\"✓ Created: age groups and age-based risk scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1534ef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FRAUD DATA - FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total new features created: 36\n",
      "Dataset shape: (151112, 60)\n",
      "\n",
      "Feature categories:\n",
      "  • Geolocation: 4 features\n",
      "  • Time-based: 15 features\n",
      "  • Transaction patterns: 6 features\n",
      "  • Device/Browser: 6 features\n",
      "  • User behavior: 4 features\n",
      "  • Demographics: 2 features\n",
      "\n",
      "================================================================================\n",
      "PART 2: CREDIT CARD DATA - FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "1. TIME-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: Time_hours, Time_days, hour_of_day, day_num\n",
      "✓ Created: time period indicators\n",
      "✓ Created: cyclical time encodings\n",
      "\n",
      "2. AMOUNT-BASED FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: amount categories, log_amount, percentiles\n",
      "\n",
      "3. V-FEATURE INTERACTIONS\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: V-feature interaction terms\n"
     ]
    }
   ],
   "source": [
    "# ==================== 7. SUMMARY OF FRAUD FEATURES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FRAUD DATA - FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "new_features = [\n",
    "    # Geolocation\n",
    "    'country', 'country_fraud_rate', 'country_tx_count', 'is_high_risk_country',\n",
    "    # Time\n",
    "    'hour_of_day', 'day_of_week', 'is_night', 'is_morning', 'is_afternoon', \n",
    "    'is_evening', 'is_weekend', 'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "    'time_since_signup_hours', 'time_since_signup_days', 'is_quick_purchase',\n",
    "    # Transaction patterns\n",
    "    'user_transaction_count', 'time_since_last_tx', 'is_rapid_tx',\n",
    "    'user_avg_purchase', 'purchase_value_ratio', 'is_unusual_amount',\n",
    "    # Device/Browser\n",
    "    'user_device_count', 'user_browser_count', 'is_multiple_devices',\n",
    "    'device_fraud_rate', 'browser_fraud_rate', 'source_fraud_rate',\n",
    "    # User behavior\n",
    "    'user_country_count', 'is_multi_country', 'user_ip_count', 'is_multi_ip',\n",
    "    # Age\n",
    "    'age_group', 'age_group_fraud_rate'\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal new features created: {len(new_features)}\")\n",
    "print(f\"Dataset shape: {fraud_data.shape}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  • Geolocation: 4 features\")\n",
    "print(f\"  • Time-based: 15 features\")\n",
    "print(f\"  • Transaction patterns: 6 features\")\n",
    "print(f\"  • Device/Browser: 6 features\")\n",
    "print(f\"  • User behavior: 4 features\")\n",
    "print(f\"  • Demographics: 2 features\")\n",
    "\n",
    "# ==================== CREDITCARD DATA FEATURE ENGINEERING ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 2: CREDIT CARD DATA - FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# The credit card data is already PCA-transformed, so we'll create minimal new features\n",
    "\n",
    "# ==================== 1. TIME FEATURES ====================\n",
    "print(\"\\n1. TIME-BASED FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Convert time to hours\n",
    "creditcard['Time_hours'] = creditcard['Time'] / 3600\n",
    "creditcard['Time_days'] = creditcard['Time'] / (3600 * 24)\n",
    "\n",
    "# Time periods (assuming 2-day period)\n",
    "creditcard['hour_of_day'] = (creditcard['Time_hours'] % 24).astype(int)\n",
    "creditcard['day_num'] = (creditcard['Time_days']).astype(int)\n",
    "\n",
    "print(\"✓ Created: Time_hours, Time_days, hour_of_day, day_num\")\n",
    "\n",
    "# Time categories\n",
    "creditcard['is_night'] = creditcard['hour_of_day'].between(0, 6).astype(int)\n",
    "creditcard['is_morning'] = creditcard['hour_of_day'].between(6, 12).astype(int)\n",
    "creditcard['is_afternoon'] = creditcard['hour_of_day'].between(12, 18).astype(int)\n",
    "creditcard['is_evening'] = creditcard['hour_of_day'].between(18, 24).astype(int)\n",
    "\n",
    "print(\"✓ Created: time period indicators\")\n",
    "\n",
    "# Cyclical encoding\n",
    "creditcard['hour_sin'] = np.sin(2 * np.pi * creditcard['hour_of_day'] / 24)\n",
    "creditcard['hour_cos'] = np.cos(2 * np.pi * creditcard['hour_of_day'] / 24)\n",
    "\n",
    "print(\"✓ Created: cyclical time encodings\")\n",
    "\n",
    "# ==================== 2. AMOUNT FEATURES ====================\n",
    "print(\"\\n2. AMOUNT-BASED FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Amount categories\n",
    "creditcard['amount_category'] = pd.cut(\n",
    "    creditcard['Amount'],\n",
    "    bins=[-0.01, 10, 50, 100, 500, 1000, 10000, np.inf],\n",
    "    labels=['<10', '10-50', '50-100', '100-500', '500-1K', '1K-10K', '>10K']\n",
    ")\n",
    "\n",
    "# Log amount (for high-value transactions)\n",
    "creditcard['log_amount'] = np.log1p(creditcard['Amount'])\n",
    "\n",
    "# Amount percentiles\n",
    "creditcard['amount_percentile'] = creditcard['Amount'].rank(pct=True)\n",
    "\n",
    "# High-value flag\n",
    "amount_95 = creditcard['Amount'].quantile(0.95)\n",
    "creditcard['is_high_value'] = (creditcard['Amount'] > amount_95).astype(int)\n",
    "\n",
    "print(\"✓ Created: amount categories, log_amount, percentiles\")\n",
    "\n",
    "# ==================== 3. V-FEATURE INTERACTIONS ====================\n",
    "print(\"\\n3. V-FEATURE INTERACTIONS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Key features identified from correlation analysis\n",
    "key_features = ['V4', 'V11', 'V12', 'V14', 'V17']\n",
    "\n",
    "# Create interaction terms for highly correlated features\n",
    "creditcard['V4_V11'] = creditcard['V4'] * creditcard['V11']\n",
    "creditcard['V4_V12'] = creditcard['V4'] * creditcard['V12']\n",
    "creditcard['V12_V14'] = creditcard['V12'] * creditcard['V14']\n",
    "\n",
    "print(\"✓ Created: V-feature interaction terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0478b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. AGGREGATE V-FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "✓ Created: V-feature statistical aggregations\n",
      "\n",
      "================================================================================\n",
      "CREDIT CARD DATA - FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total new features created: 16\n",
      "Dataset shape: (284807, 54)\n",
      "\n",
      "Feature categories:\n",
      "  • Time-based: 7 features\n",
      "  • Amount-based: 3 features\n",
      "  • V-interactions: 3 features\n",
      "  • V-aggregations: 3 features\n"
     ]
    }
   ],
   "source": [
    "# ==================== 4. AGGREGATE FEATURES ====================\n",
    "print(\"\\n4. AGGREGATE V-FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get all V columns\n",
    "v_cols = [col for col in creditcard.columns if col.startswith('V') and len(col) <= 3]\n",
    "\n",
    "# Statistical aggregations\n",
    "creditcard['V_mean'] = creditcard[v_cols].mean(axis=1)\n",
    "creditcard['V_std'] = creditcard[v_cols].std(axis=1)\n",
    "creditcard['V_max'] = creditcard[v_cols].max(axis=1)\n",
    "creditcard['V_min'] = creditcard[v_cols].min(axis=1)\n",
    "creditcard['V_range'] = creditcard['V_max'] - creditcard['V_min']\n",
    "\n",
    "print(\"✓ Created: V-feature statistical aggregations\")\n",
    "\n",
    "# ==================== 5. SUMMARY OF CREDIT CARD FEATURES ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREDIT CARD DATA - FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "new_cc_features = [\n",
    "    'Time_hours', 'Time_days', 'hour_of_day', 'is_night', 'is_morning',\n",
    "    'hour_sin', 'hour_cos', 'log_amount', 'amount_percentile', 'is_high_value',\n",
    "    'V4_V11', 'V4_V12', 'V12_V14', 'V_mean', 'V_std', 'V_range'\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal new features created: {len(new_cc_features)}\")\n",
    "print(f\"Dataset shape: {creditcard.shape}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  • Time-based: 7 features\")\n",
    "print(f\"  • Amount-based: 3 features\")\n",
    "print(f\"  • V-interactions: 3 features\")\n",
    "print(f\"  • V-aggregations: 3 features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073fc15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING ENGINEERED DATASETS\n",
      "================================================================================\n",
      "✓ Saved: fraud_data_engineered.csv ((151112, 60))\n",
      "✓ Saved: creditcard_engineered.csv ((284807, 54))\n"
     ]
    }
   ],
   "source": [
    "# ==================== SAVE ENGINEERED DATA ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING ENGINEERED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save fraud data\n",
    "fraud_data.to_csv('../data/processed/fraud_data_engineered.csv', index=False)\n",
    "print(f\"✓ Saved: fraud_data_engineered.csv ({fraud_data.shape})\")\n",
    "\n",
    "# Save credit card data\n",
    "creditcard.to_csv('../data/processed/creditcard_engineered.csv', index=False)\n",
    "print(f\"✓ Saved: creditcard_engineered.csv ({creditcard.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceeba56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE PREVIEW\n",
      "================================================================================\n",
      "\n",
      "Fraud Data - Top Features by Correlation with Target:\n",
      "class                      1.000000\n",
      "device_fraud_rate          0.932580\n",
      "is_quick_purchase          0.714120\n",
      "is_very_new_account        0.663749\n",
      "month                      0.310112\n",
      "time_since_signup_hours    0.257888\n",
      "time_since_signup_days     0.257888\n",
      "day_of_month               0.160319\n",
      "day_sin                    0.026106\n",
      "source_fraud_rate          0.020728\n",
      "day_of_week                0.018939\n",
      "browser_fraud_rate         0.017171\n",
      "is_weekend                 0.014139\n",
      "hour_cos                   0.010131\n",
      "day_cos                    0.008301\n",
      "Name: class, dtype: float64\n",
      "\n",
      "Credit Card Data - Top Features by Correlation with Target:\n",
      "Class      1.000000\n",
      "V12_V14    0.582719\n",
      "V4_V12     0.537021\n",
      "V4_V11     0.445713\n",
      "V17        0.326481\n",
      "V_mean     0.316330\n",
      "V14        0.302544\n",
      "V12        0.260593\n",
      "V_std      0.250839\n",
      "V_min      0.220023\n",
      "V10        0.216883\n",
      "V_range    0.202200\n",
      "V16        0.196539\n",
      "V3         0.192961\n",
      "V7         0.187257\n",
      "Name: Class, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "✓ Ready for data transformation and modeling\n",
      "✓ Next step: Data preprocessing (scaling, encoding, train-test split)\n"
     ]
    }
   ],
   "source": [
    "# ==================== FEATURE IMPORTANCE PREVIEW ====================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFraud Data - Top Features by Correlation with Target:\")\n",
    "fraud_numerical = fraud_data.select_dtypes(include=[np.number])\n",
    "if 'class' in fraud_numerical.columns:\n",
    "    fraud_corr = fraud_numerical.corr()['class'].abs().sort_values(ascending=False)\n",
    "    print(fraud_corr.head(15))\n",
    "\n",
    "print(\"\\nCredit Card Data - Top Features by Correlation with Target:\")\n",
    "cc_numerical = creditcard.select_dtypes(include=[np.number])\n",
    "if 'Class' in cc_numerical.columns:\n",
    "    cc_corr = cc_numerical.corr()['Class'].abs().sort_values(ascending=False)\n",
    "    print(cc_corr.head(15))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n✓ Ready for data transformation and modeling\")\n",
    "print(\"✓ Next step: Data preprocessing (scaling, encoding, train-test split)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
